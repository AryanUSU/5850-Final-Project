{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32f9a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d671f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top 25 subreddit names to read files\n",
    "subredditNames = [\"AmItheAsshole\", \"AskReddit\", \"Damnthatsinteresting\", \"DestinyTheGame\", \n",
    "                  \"Home\", \"LivestreamFail\", \"NoStupidQuestions\", \"PublicFreakout\", \"Unexpected\", \n",
    "                  \"WhitePeopleTwitter\", \"antiwork\", \"diablo4\", \"explainlikeimfive\", \"facepalm\", \n",
    "                  \"funny\", \"gaming\", \"interestingasfuck\", \"leagueoflegends\", \"mildlyinfuriating\", \n",
    "                  \"movies\", \"pcmasterrace\", \"pics\", \"therewasanattempt\", \"videos\", \"worldnews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed736b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the true count of tokens in subreddit\n",
    "def UpdateTrueCount(trueCount, token):\n",
    "    if token in trueCount:\n",
    "        trueCount[token] += 1\n",
    "    else:\n",
    "        trueCount[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9295b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FalsePositiveRate(buffer, trueCount, m, k):\n",
    "    fpr = 0\n",
    "    for key in buffer.keys():\n",
    "        if trueCount[key] < (m / k):\n",
    "            fpr += 1\n",
    "        \n",
    "    return fpr / len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65d09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Misra-Gries algorithm\n",
    "def MisraGries(token, buffer, k): \n",
    "    if token in buffer:\n",
    "        buffer[token] = buffer[token] + 1\n",
    "    elif len(buffer) < k-1:\n",
    "        buffer[token] = 1\n",
    "    else:\n",
    "        keys = list(buffer.keys())\n",
    "        for key in keys:\n",
    "            buffer[key] -= 1\n",
    "            if buffer[key] == 0:\n",
    "                del buffer[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76e36eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs uncommenting if not downloaded\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "stop.append(\"like\")\n",
    "stop.append(\"get\")\n",
    "\n",
    "# Checks if a word is a common word, url, and lemmatizes the word\n",
    "def CommonWord(word):\n",
    "    isCommonWord = False\n",
    "\n",
    "    if len(word) <= 1 or word in stop: isCommonWord = True\n",
    "    elif word.startswith('https://') or word.startswith('http://'): isCommonWord = True\n",
    "\n",
    "    # Performs Lemmatization\n",
    "    word = wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "    return word, isCommonWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab1e11ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmItheAsshole\n",
      "0.7096774193548387 \n",
      "\n",
      "AskReddit\n",
      "0.8108552631578947 \n",
      "\n",
      "Damnthatsinteresting\n",
      "0.82174688057041 \n",
      "\n",
      "DestinyTheGame\n",
      "0.7767722473604827 \n",
      "\n",
      "Home\n",
      "0.8130081300813008 \n",
      "\n",
      "LivestreamFail\n",
      "0.6882217090069284 \n",
      "\n",
      "NoStupidQuestions\n",
      "0.7209821428571429 \n",
      "\n",
      "PublicFreakout\n",
      "0.7712177121771218 \n",
      "\n",
      "Unexpected\n",
      "0.8213333333333334 \n",
      "\n",
      "WhitePeopleTwitter\n",
      "0.7211981566820277 \n",
      "\n",
      "antiwork\n",
      "0.730593607305936 \n",
      "\n",
      "diablo4\n",
      "0.7674418604651163 \n",
      "\n",
      "explainlikeimfive\n",
      "0.8360655737704918 \n",
      "\n",
      "facepalm\n",
      "0.6588235294117647 \n",
      "\n",
      "funny\n",
      "0.6917808219178082 \n",
      "\n",
      "gaming\n",
      "0.779467680608365 \n",
      "\n",
      "interestingasfuck\n",
      "0.7707129094412332 \n",
      "\n",
      "leagueoflegends\n",
      "0.8372093023255814 \n",
      "\n",
      "mildlyinfuriating\n",
      "0.8069883527454242 \n",
      "\n",
      "movies\n",
      "0.7272727272727273 \n",
      "\n",
      "pcmasterrace\n",
      "0.7639639639639639 \n",
      "\n",
      "pics\n",
      "0.8300907911802854 \n",
      "\n",
      "therewasanattempt\n",
      "0.7785714285714286 \n",
      "\n",
      "videos\n",
      "0.7808471454880295 \n",
      "\n",
      "worldnews\n",
      "0.7780487804878049 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subreddit in subredditNames:\n",
    "    print(subreddit)\n",
    "    with open('Reddit_Comments/' + subreddit + '.txt') as f:\n",
    "        buffer = {}\n",
    "        trueCount = {}\n",
    "        k = 801\n",
    "        m = 0\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            random.shuffle(tokens) # Randomizes the tokens\n",
    "            for token in tokens:\n",
    "                token, isCommonWord = CommonWord(token)\n",
    "                if isCommonWord: continue\n",
    "                \n",
    "                m += 1\n",
    "                MisraGries(token, buffer, k)\n",
    "                UpdateTrueCount(trueCount, token)\n",
    "            \n",
    "#     print(buffer)\n",
    "#     nHighestCount(trueCount, 10)\n",
    "    print(FalsePositiveRate(buffer, trueCount, m, k), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ff19afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints out the first n tokens with the highest count in a subreddit\n",
    "def nHighestCount(trueCount, n):\n",
    "    # Sort the trueCount\n",
    "    sortedDict = {k: v for k, v in sorted(trueCount.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # Print first n values\n",
    "    print({k: sortedDict[k] for k in list(sortedDict)[:n]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
